# Embedding and RAG Policies for Job Application Screening System

## Document Categories and Collection Strategy

### 1. System-Internal Documents (Ground Truth)
These documents form the knowledge base for evaluation:

#### Job Descriptions Collection
- **Purpose**: Ground truth for CV evaluation
- **Documents**: Multiple job description variations for the role
- **Key Content**: Technical requirements, experience levels, responsibilities
- **Embedding Strategy**: Chunk by skills, requirements, and qualifications
- **Collection Name**: `job_descriptions`

#### Case Study Brief Collection
- **Purpose**: Ground truth for project report evaluation
- **Documents**: This case study brief and any related project requirements
- **Key Content**: Technical requirements, evaluation criteria, project specifications
- **Embedding Strategy**: Chunk by objectives, technical requirements, and deliverables
- **Collection Name**: `case_studies`

#### CV Scoring Rubric Collection
- **Purpose**: Evaluation framework for candidate CVs
- **Documents**: CV evaluation parameters and scoring guides
- **Key Content**: Technical skills match, experience level, achievements, cultural fit
- **Embedding Strategy**: Chunk by parameter categories and scoring criteria
- **Collection Name**: `cv_scoring_rubrics`

#### Project Scoring Rubric Collection
- **Purpose**: Evaluation framework for project deliverables
- **Documents**: Project evaluation parameters and scoring guides
- **Key Content**: Correctness, code quality, resilience, documentation, creativity
- **Embedding Strategy**: Chunk by parameter categories and scoring criteria
- **Collection Name**: `project_scoring_rubrics`

### 2. Candidate Documents (Evaluation Targets)
These documents are processed but not stored in vector DB:

#### Candidate CVs
- **Purpose**: Candidate's resume to be evaluated
- **Processing**: Parse into structured data, extract key sections
- **Key Sections**: Experience, technical skills, achievements, education
- **Storage**: File system with metadata, not in vector DB

#### Project Reports
- **Purpose**: Candidate's project implementation report
- **Processing**: Parse into structured data, extract technical details
- **Key Sections**: Implementation approach, code quality, architecture, results
- **Storage**: File system with metadata, not in vector DB

## Embedding Configuration

### Chunking Strategy
- **Job Descriptions**: 500-800 word chunks focused on specific requirements
- **Case Studies**: 400-600 word chunks by objective/requirement
- **Rubrics**: 200-400 word chunks by evaluation parameter
- **Overlap**: 100 words between chunks for context continuity

### Metadata Schema
```json
{
  "document_type": "job_description|case_study|cv_rubric|project_rubric",
  "category": "technical_skills|experience|requirements|evaluation_criteria",
  "weight": "numeric_weight_for_scoring",
  "source_file": "original_filename",
  "chunk_id": "unique_chunk_identifier",
  "created_at": "timestamp"
}
```

### Retrieval Strategy
- **CV Evaluation**: Retrieve from job_descriptions + cv_scoring_rubrics
- **Project Evaluation**: Retrieve from case_studies + project_scoring_rubrics
- **Similarity Threshold**: 0.7 for relevant content retrieval
- **Max Results**: Top 5 most relevant chunks per query

## RAG Context Injection Rules

### CV Evaluation Context
1. Retrieve relevant job requirements based on CV content
2. Retrieve scoring rubric sections for identified skill areas
3. Inject into CV evaluation prompt with clear section labels
4. Prioritize technical skills and experience level matching

### Project Evaluation Context
1. Retrieve case study requirements based on project content
2. Retrieve project scoring rubric sections for identified implementation areas
3. Inject into project evaluation prompt with requirement-comparison format
4. Prioritize correctness, code quality, and resilience criteria

### Context Formatting
```
=== RELEVANT REQUIREMENTS ===
[Retrieved job description/case study content]

=== EVALUATION CRITERIA ===
[Retrieved scoring rubric content]

=== CANDIDATE SUBMISSION ===
[CV or project content to evaluate]
```

## Quality Assurance

### Embedding Quality Checks
- Verify all chunks contain meaningful content
- Ensure metadata completeness and accuracy
- Validate chunk sizes stay within optimal range
- Test retrieval relevance with sample queries

### RAG Performance Monitoring
- Track retrieval accuracy and relevance scores
- Monitor context injection completeness
- Log evaluation result consistency
- Measure similarity threshold effectiveness

## Security and Privacy

### Data Handling
- Candidate documents (CVs, project reports) are NOT stored in vector DB
- Only system-internal documents are embedded and stored
- Candidate data is processed in memory only
- File storage uses secure, access-controlled directories

### Access Control
- Vector collections are read-only after initial ingestion
- No candidate PII is embedded or stored in search indexes
- All processing logs exclude sensitive candidate information
- Document access is audited and logged

## Scaling Considerations

### Collection Management
- Separate collections by document type for better performance
- Implement collection versioning for rubric updates
- Use batch processing for large document sets
- Implement caching for frequently accessed rubric sections

### Performance Optimization
- Pre-embed static documents (job descriptions, rubrics)
- Use semantic search for better context matching
- Implement query result caching for repeated evaluation patterns
- Monitor and optimize embedding model performance

## Error Handling

### Ingestion Errors
- Retry failed embeddings with exponential backoff
- Log all ingestion failures with detailed error context
- Implement validation checks for document quality
- Provide fallback options for embedding service outages

### Retrieval Errors
- Graceful degradation when vector DB is unavailable
- Fallback to keyword-based search if needed
- Implement timeout handling for retrieval operations
- Cache critical rubric information for offline use